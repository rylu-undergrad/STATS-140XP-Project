# -*- coding: utf-8 -*-
"""STATS 140XP Final Project

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1U0jze_CgcFP_Vejop96RkfXaIap1JxY8

# Import Libraries
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px

import re
import string

# VADER Sentiment Analyzer from NLTK
from nltk.sentiment import SentimentAnalyzer
from nltk.sentiment.util import *
from nltk.sentiment.vader import SentimentIntensityAnalyzer
nltk.download('vader_lexicon')

from wordcloud import WordCloud, ImageColorGenerator
from nltk.corpus import stopwords

import nltk
nltk.download('stopwords')

"""# Import Dataset"""

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv('/content/drive/MyDrive/STATS 140XP Final Project/tweets_01-08-2021.csv')
print(df.shape)
df.head(10)

# Select only the columns that will be used
df = df.drop(["id"], axis = 1)

df.head(10)

# Exploring more features
df["date"]= pd.to_datetime(df["date"])
df["Year"]= df['date'].dt.year
df["Month"]= df['date'].dt.month
df["Date_no"]= df['date'].dt.day
df["Hour"]= df['date'].dt.hour
df["Day"]= df.date.dt.strftime("%A")
df['date_dmy'] = df['date'].apply(lambda x: x.strftime("%Y-%m-%d"))
df['sentence_length'] = df.text.apply(len)

"""**Clean up the text**"""

def clean_text(text):
    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation
    and remove words containing numbers.'''
    text = str(text).lower()
    text = re.sub('&amp', '', text)
    text = re.sub('rt @\D\w*:', '', text)
    text = re.sub('\[.*?\]', '', text)
    text = re.sub('https?://\S+|www\.\S+', '', text)
    text = re.sub('<.*?>+', '', text)
    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)
    text = re.sub('\n', '', text)
    text = re.sub('\w*\d\w*', '', text)
    return text

df['text_clean'] = df['text'].apply(clean_text)
df['text_clean'].head(10)

"""# EDA

**Number of tweets by year**
"""

df["date"] = pd.to_datetime(df["date"])
df["date"].apply(lambda x: x.year)

colors = []
for i in range(2021-2009):
    x = 0.9-0.07*i
    c = (x,x,0.5)
    colors.append(c)

bar = df["date"].apply(lambda x: x.year).value_counts().sort_index().plot.bar(figsize = (7,5), color = colors)
plt.title("Number of tweets by year\n", fontsize=20)
bar.tick_params(labelsize=5)
plt.axvline(8, 0 ,1, color = "grey", lw = 3)
plt.text(7, 8800, "President", fontsize = 15, color = "red")
bar.tick_params(labelsize=18)
plt.show()

"""It seems that Trump wasn't really active on Twitter before his presidency, especially during 2017 when he got elected. But we noticed a trend that starting in 2019, he has been tweeting a lot more and his peak time was in 2020.

**Compound Scores Analysis**
"""

# Calculate the polarity of the tweets of Trump with NLTK
from nltk.sentiment.vader import SentimentIntensityAnalyzer
sentiment = SentimentIntensityAnalyzer()

df["polarity"] = df["text_clean"].apply(lambda x: sentiment.polarity_scores(x))

df["pos"] = df["polarity"].apply(lambda x: x["pos"])
df["neg"] = df["polarity"].apply(lambda x: x["neg"])
df["compound"] = df["polarity"].apply(lambda x: x["compound"])

conditions = [
    (df['compound'] > 0),
    (df['compound'] < 0)
]
choices = ['positive','negative']

df['label'] = np.select(conditions, choices)

df.head()

"""**Distribution of Compound Scores**"""

fig, ax = plt.subplots(figsize = (10,7), dpi = 80)
font_color = "black"
sns.histplot(df['compound'], ax = ax, color = 'r', binwidth = 0.10, stat = 'density')
plt.legend(['Compound Score'])
ax.set_xlim(-1,1)

ax.set_title('Distribution of Compound Scores for Trump Tweets', fontsize = 12, fontweight = 'bold', color = font_color)
ax.set_xlabel('Score', fontsize = 10, fontweight = 'bold', color = font_color)
ax.set_ylabel('Density', fontsize = 10, fontweight = 'bold', color = font_color)
plt.show()

"""Trump's Tweets tend to be on the positive side.

# Sentiment Analysis
"""

fig, ax = plt.subplots()
ax = sns.barplot(data = df, x = 'label', y = 'retweets')
ax.set_title('Median Number of Retweets Per Tweet by Sentiment Polarity')
ax.set_xlabel('Sentiment', fontsize = 20)
ax.set_ylabel('Number of Retweets', fontsize = 20)
ax.tick_params(axis='both', which='major', labelsize = 20)

print (f"Ratio of Retweets Negative:Positive Tweets: {round(df.loc[df['label'] == 'negative', 'retweets'].values[0]/df.loc[df['label'] == 'positive', 'retweets'].values[0],5)}")

fig, ax = plt.subplots()
ax = sns.barplot(data = df, x = 'label', y = 'favorites')
ax.set_title('Median Number of Favorites Per Tweet by Sentiment Polarity')
ax.set_xlabel('Sentiment', fontsize = 20)
ax.set_ylabel('Number of Favorites', fontsize = 20)
ax.tick_params(axis='both', which='major', labelsize = 20)

print (f"Ratio of Retweets Negative:Positive Tweets: {round(df.loc[df['label'] == 'negative', 'favorites'].values[0]/df.loc[df['label'] == 'positive', 'favorites'].values[0],5)}")

temp = pd.read_csv("")

"""# Relationships Comparison

**Heatmap for Retweets, Favorites, and Compound**
"""

# Sentiment (Positive and Negative mean joined) aka Compound
df["year_month"] = df["date"].apply(lambda x: str(x.year)+"-"+str(x.month))
df["year_month"] = pd.to_datetime(df["year_month"])
year_month = pd.pivot_table(df, values = "compound", index = "year_month", aggfunc = "mean")

plt.figure(figsize = (7,5))
sns.heatmap(df[["retweets","favorites", "compound"]].corr(), annot = True, cmap="YlGnBu")
plt.title("Correlation between Retweets, Favorites and Compound\n", fontsize = 15)
plt.show()

"""- It seems that there is a high correlation between Retweets and Favorites which means if the Retweets number is high for a post, then the Favorites number is also likely to be high.

- But the correlation numbers between Sentiment with Retweets and Favorites are both low.

**Scatterplots of their relationship**
"""

# Between Retweets and Favorites
plt.figure(figsize=(7,5))
sns.scatterplot(data = df, x = "retweets", y = "favorites")
plt.title("Relation between Retweets and Favorites", fontsize = 15)
plt.show()

# Between Retweets and Sentiment
plt.figure(figsize=(7,5))
sns.scatterplot(data = df, x = "compound", y = "retweets")
plt.title("Relation between Retweets and Compound", fontsize = 15)
plt.show()

# Between Favorites and Sentiment
plt.figure(figsize=(7,5))
sns.scatterplot(data = df, x = "compound", y = "favorites", alpha = 0.5)
plt.title("Relation between Favorites and Compound", fontsize = 15)
plt.show()

"""**Conclusion:** The positivity and/or negativity of Trump tweets don't seem to have an influence on the number of retweets/favorites.

#code for analyzing each sentiment
import matplotlib.pyplot as plt
import seaborn as sns

fear = sum(data['fear'])
anger = sum(data['anger'])
anticipation = sum(data['anticipation'])
trust = sum(data['trust'])
surprise = sum(data['surprise'])
sadness = sum(data['sadness'])
disgust = sum(data['disgust'])
joy = sum(data['joy'])

sentiments = ['fear', 'anger', 'anticipation', 'trust', 'surprise', 'sadness', 'disgust', 'joy']
scores = [fear, anger, anticipation, trust, surprise, sadness, disgust, joy]
colors = ['firebrick', 'red', 'orange', 'yellow', 'green', 'blue', 'indigo', 'purple']


plt.figure(figsize=(8, 4))
plt.bar(sentiments, scores, color=colors)

# Adding titles and labels
plt.title('Cummulative Sentiment Score')
plt.xlabel('Sentiments')
plt.ylabel('Cummulative Scores')

mean = [fear/56571, anger/56571,
        anticipation/56571,
        trust/56571, surprise/56571,
        sadness/56571, disgust/56571,
        joy/56571]

plt.figure(figsize=(8, 4))
plt.bar(sentiments, mean, color=colors)

# Adding titles and labels
plt.title('Mean Sentiment Score')
plt.xlabel('Sentiments')
plt.ylabel('Mean Scores')

# Habits & Tweeting Patterns

**Devices used for Tweets Analysis**
"""

import plotly.express as px

df['sentence_length'] = df.text.apply(len)

data = df[df['text'].apply(len) != 0]
top_devices = data.groupby('device')['text'].count().sort_values(ascending=False).index.tolist()

data = data[data['device'].apply(lambda x: x in top_devices)]
fig = px.histogram(data, x="sentence_length", color="device", opacity=0.75)

fig.update_layout(
    legend_title_font_color="blue",
    hovermode='x',
    xaxis_title="Text Length",
    yaxis_title="Number of Tweets",
    title="Devices used for Tweets"
)

fig.show()

"""Trump tends to send longer tweets and tweets the most from an Iphone.

**Trump's Tweets by Hour**
"""

def format_hour(h: int):
    h = str(h)
    if len(h) == 1:
        h = '0'+h
    h = h+ ":00"
    return h

hourly = df.groupby('Hour')['text'].count()
hourly = pd.DataFrame(hourly).reset_index()
hourly.columns =['Hour of Day',"Number of Tweets"]
hourly['Hour of Day'] = hourly['Hour of Day'].apply(format_hour)

fig= px.bar_polar(hourly,'Number of Tweets', theta = 'Hour of Day', color = "Number of Tweets",
                 color_discrete_sequence = px.colors.sequential.Plasma_r)

fig.update_layout(
    font_color = "black",
    title_font_color = "black",
    legend_title_font_color = "blue",
    plot_bgcolor = '#dff0f5',
    hovermode = 'x',
    title = "Total numbers of Trump's Tweet by Hour"
)

fig.show()

"""He does most of his tweeting from 12:00 PM and 7:00 PM and 8:00 PM. Although it seems that in total, he tweets at every hour regardless of day and night, he is really active on Twitter.

# Frequent Words Used

**Clean up the text**
"""

def clean_text(text):
    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation
    and remove words containing numbers.'''
    text = str(text).lower()
    text = re.sub('&amp', '', text)
    text = re.sub('rt @\D\w*:', '', text)
    text = re.sub('\[.*?\]', '', text)
    text = re.sub('https?://\S+|www\.\S+', '', text)
    text = re.sub('<.*?>+', '', text)
    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)
    text = re.sub('\n', '', text)
    text = re.sub('\w*\d\w*', '', text)
    return text

df['message_clean'] = df['text'].apply(clean_text)
df['text'].head(10)

"""**Sentiment Analyzer NLP**"""

sentiment_analyzer = SentimentIntensityAnalyzer()

def sentiment_scores(message_clean):
  score = sentiment_analyzer.polarity_scores(message_clean)
  return score

df['polarity_scores'] = df.text.apply(sentiment_scores)
df['compound'] = df.polarity_scores.apply(lambda x: x['compound'])
df['neg'] = df.polarity_scores.apply(lambda x: x['neg'])
df['pos'] = df.polarity_scores.apply(lambda x: x['pos'])

conditions = [
    (df['compound'] > 0),
    (df['compound'] < 0)
]
choices = ['positive','negative']

df['vader_compound'] = np.select(conditions, choices)

# Grouping sentiment scores by day to get average daily sentiment scores
dt_sentiment_dmy = df.vader_compound.groupby(df.date_dmy).mean().reset_index()

dt_sentiment_dmy["date_dmy"]= pd.to_datetime(dt_sentiment_dmy["date_dmy"])
dt_sentiment_dmy["year"]= dt_sentiment_dmy['date_dmy'].dt.year

"""**Descriptive Statistics**"""

data = df[df['text'].apply(len) != 0]
top_devices = data.groupby('device')['text'].count().sort_values(ascending=False).index.tolist()

data = data[data['device'].apply(lambda x: x in top_devices)]

def flatten_list(l):
    return [x for y in l for x in y]

# color coding our wordclouds
def red_color_func(word, font_size, position, orientation, random_state=None,**kwargs):
    return f"hsl(0, 100%, {random.randint(25, 75)}%)"

def green_color_func(word, font_size, position, orientation, random_state=None,**kwargs):
    return f"hsl({random.randint(90, 150)}, 100%, 30%)"

def generate_word_clouds(neg_doc, pos_doc):
 # Display the generated image:

  fig, axes = plt.subplots(1,2, figsize=(20,10))
  wordcloud_neg = WordCloud(max_font_size=50, max_words=100, background_color="white").generate(" ".join(neg_doc))
  axes[0].imshow(wordcloud_neg.recolor(color_func=red_color_func, random_state=3), interpolation='bilinear')
  axes[0].set_title("Negative Tweets", fontsize = 30)
  axes[0].axis("off")

  wordcloud_pos = WordCloud(max_font_size=50, max_words=100, background_color="white").generate(" ".join(pos_doc))
  axes[1].imshow(wordcloud_pos.recolor(color_func=green_color_func, random_state=3), interpolation='bilinear')
  axes[1].set_title("Positive Tweets", fontsize = 30)
  axes[1].axis("off")

  plt.tight_layout()
  plt.show();

sentiment_sorted = data.sort_values('favorites', ascending = False)
sentiment_sorted = data.sort_values('retweets', ascending = False)
positive_top_100 = df[df['vader_compound'] == "positive"].iloc[:100]
negative_top_100 = df[df['vader_compound'] == "negative"].iloc[:100]

cleanup = lambda x: [y for y in x.split() if y not in stopwords.words('english')]
neg_doc = flatten_list(negative_top_100['text_clean'].apply(cleanup))
pos_doc = flatten_list(positive_top_100['text_clean'].apply(cleanup))

generate_word_clouds(neg_doc, pos_doc)

print(df["device"].value_counts())

def flatten_list(l):
    return [x for y in l for x in y]

# color coding our wordclouds
def red_color_func(word, font_size, position, orientation, random_state=None,**kwargs):
    return f"hsl(0, 100%, {random.randint(25, 75)}%)"

def green_color_func(word, font_size, position, orientation, random_state=None,**kwargs):
    return f"hsl({random.randint(90, 150)}, 100%, 30%)"

def generate_word_clouds(neg_doc, pos_doc):
 # Display the generated image:

  fig, axes = plt.subplots(1,2, figsize=(20,10))
  wordcloud_neg = WordCloud(max_font_size=50, max_words=100, background_color="white").generate(" ".join(neg_doc))
  axes[0].imshow(wordcloud_neg.recolor(color_func=red_color_func, random_state=3), interpolation='bilinear')
  axes[0].set_title("Tweets from Android", fontsize = 30)
  axes[0].axis("off")

  wordcloud_pos = WordCloud(max_font_size=50, max_words=100, background_color="white").generate(" ".join(pos_doc))
  axes[1].imshow(wordcloud_pos.recolor(color_func=green_color_func, random_state=3), interpolation='bilinear')
  axes[1].set_title("Tweets from iPhone", fontsize = 30)
  axes[1].axis("off")

  plt.tight_layout()
  plt.show();

sentiment_sorted = data.sort_values('favorites', ascending = False)
sentiment_sorted = data.sort_values('retweets', ascending = False)
iphone_top_100 = df[(df['device'] == "Twitter for iPhone") & (df['vader_compound'] == "positive")].iloc[:100]
android_top_100 = df[(df['device'] == "Twitter for Android") & (df['vader_compound'] == "negative")].iloc[:100]

cleanup = lambda x: [y for y in x.split() if y not in stopwords.words('english')]
neg_doc = flatten_list(android_top_100['text_clean'].apply(cleanup))
pos_doc = flatten_list(iphone_top_100['text_clean'].apply(cleanup))

generate_word_clouds(neg_doc, pos_doc)